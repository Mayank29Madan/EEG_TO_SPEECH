{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11505254,
          "sourceType": "datasetVersion",
          "datasetId": 7213641
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# print(\"Go\")\n",
        "\n",
        "# !pip install gdown\n",
        "# # !gdown \"https://drive.google.com/uc?id=1NplEy9bpy4aJHORtN4VXapo0RQ9nPtRw\"\n",
        "\n",
        "# # !gdown \"https://drive.google.com/uc?id=1JMwEHSTcRn71gBc7JjJG8GhmmqcP8Gpk\"\n",
        "\n",
        "# !gdown \"https://drive.google.com/file/d/18jbEBrtPm0CCTlu_XgeNIpviOcemRsLB\""
      ],
      "metadata": {
        "trusted": true,
        "id": "ITD-iYB-zlYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e050c221-5dbd-4db7-b96b-ced5391b1606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.11/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/18jbEBrtPm0CCTlu_XgeNIpviOcemRsLB\n",
            "To: /content/18jbEBrtPm0CCTlu_XgeNIpviOcemRsLB\n",
            "92.5kB [00:00, 3.21MB/s]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import scipy.signal as signal\n",
        "from scipy.stats import skew, kurtosis\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "class EEGPreprocessor:\n",
        "    \"\"\"\n",
        "    Implements the comprehensive EEG preprocessing pipeline as described.\n",
        "    Transforms raw EEG signals into clean, standardized data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fs = 1000  # Sampling frequency (1000 Hz)\n",
        "\n",
        "    def remove_dc_offset(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Removes DC offset using a high-pass filter (2nd-order Butterworth, 0.1Hz cutoff)\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Raw EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            EEG data with DC offset removed\n",
        "        \"\"\"\n",
        "        b, a = signal.butter(2, 0.1 / (self.fs / 2), 'highpass')\n",
        "        return signal.filtfilt(b, a, eeg_data, axis=1)\n",
        "\n",
        "    def bandpass_filter(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Applies a 4th-order Butterworth bandpass filter (0.5-70Hz)\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            Bandpass filtered EEG data\n",
        "        \"\"\"\n",
        "        b, a = signal.butter(4, [0.5 / (self.fs / 2), 70 / (self.fs / 2)], 'bandpass')\n",
        "        return signal.filtfilt(b, a, eeg_data, axis=1)\n",
        "\n",
        "    def notch_filter(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Applies IIR notch filter centered at 50Hz (power line frequency)\n",
        "        Note: Using 50Hz for power line interference since this appears to be from a European or Asian dataset.\n",
        "              For US datasets, 60Hz would be used instead.\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            Notch filtered EEG data\n",
        "        \"\"\"\n",
        "        b, a = signal.iirnotch(50, 30, self.fs)\n",
        "        return signal.filtfilt(b, a, eeg_data, axis=1)\n",
        "\n",
        "    def remove_artifacts(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Removes artifacts using robust thresholding based on median absolute deviation (MAD)\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            EEG data with artifacts removed\n",
        "        \"\"\"\n",
        "        cleaned_data = np.copy(eeg_data)\n",
        "\n",
        "        for ch in range(eeg_data.shape[0]):\n",
        "            channel_data = eeg_data[ch, :]\n",
        "            median = np.median(channel_data)\n",
        "            mad = np.median(np.abs(channel_data - median))\n",
        "\n",
        "            # Scale factor for MAD to approximate standard deviation\n",
        "            threshold = 5 * 1.4826 * mad\n",
        "\n",
        "            # Identify artifacts\n",
        "            artifact_mask = np.abs(channel_data - median) > threshold\n",
        "\n",
        "            if np.any(artifact_mask):\n",
        "                # Get indices of clean samples (not artifacts)\n",
        "                clean_indices = np.where(~artifact_mask)[0]\n",
        "\n",
        "                # Get indices of artifact samples\n",
        "                artifact_indices = np.where(artifact_mask)[0]\n",
        "\n",
        "                # Interpolate clean values onto artifact positions\n",
        "                for idx in artifact_indices:\n",
        "                    # Find nearest clean samples before and after\n",
        "                    before = clean_indices[clean_indices < idx]\n",
        "                    after = clean_indices[clean_indices > idx]\n",
        "\n",
        "                    if len(before) > 0 and len(after) > 0:\n",
        "                        # Linear interpolation between nearest clean points\n",
        "                        before_idx = np.max(before)\n",
        "                        after_idx = np.min(after)\n",
        "                        before_val = channel_data[before_idx]\n",
        "                        after_val = channel_data[after_idx]\n",
        "\n",
        "                        # Linear interpolation\n",
        "                        weight = (idx - before_idx) / (after_idx - before_idx)\n",
        "                        cleaned_data[ch, idx] = before_val * (1 - weight) + after_val * weight\n",
        "                    elif len(before) > 0:\n",
        "                        # Use last clean value\n",
        "                        cleaned_data[ch, idx] = channel_data[np.max(before)]\n",
        "                    elif len(after) > 0:\n",
        "                        # Use next clean value\n",
        "                        cleaned_data[ch, idx] = channel_data[np.min(after)]\n",
        "\n",
        "        return cleaned_data\n",
        "\n",
        "    def common_average_reference(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Applies common average re-referencing\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            Common average re-referenced EEG data\n",
        "        \"\"\"\n",
        "        return eeg_data - np.mean(eeg_data, axis=0, keepdims=True)\n",
        "\n",
        "    def normalize(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Normalizes EEG data using robust scaling (median, IQR)\n",
        "        Falls back to z-score if IQR is zero\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape (n_channels, n_samples)\n",
        "\n",
        "        Returns:\n",
        "            Normalized EEG data\n",
        "        \"\"\"\n",
        "        normalized_data = np.zeros_like(eeg_data, dtype=np.float32)\n",
        "\n",
        "        for ch in range(eeg_data.shape[0]):\n",
        "            channel_data = eeg_data[ch, :]\n",
        "            median = np.median(channel_data)\n",
        "            q75, q25 = np.percentile(channel_data, [75, 25])\n",
        "            iqr = q75 - q25\n",
        "\n",
        "            if iqr > 1e-10:  # Use robust scaling if IQR is non-zero\n",
        "                normalized_data[ch, :] = (channel_data - median) / iqr\n",
        "            else:  # Fall back to z-score normalization\n",
        "                mean = np.mean(channel_data)\n",
        "                std = np.std(channel_data)\n",
        "                if std > 1e-10:\n",
        "                    normalized_data[ch, :] = (channel_data - mean) / std\n",
        "                else:\n",
        "                    normalized_data[ch, :] = channel_data - mean  # Just center if std is too small\n",
        "\n",
        "        return normalized_data\n",
        "\n",
        "    def process(self, eeg_data, verbose=False):\n",
        "        \"\"\"\n",
        "        Applies the complete preprocessing pipeline\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Raw EEG data of shape (n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Fully preprocessed EEG data\n",
        "        \"\"\"\n",
        "        # Convert to numpy if it's a tensor\n",
        "        if isinstance(eeg_data, torch.Tensor):\n",
        "            eeg_data = eeg_data.numpy()\n",
        "\n",
        "        # Apply each preprocessing step sequentially with optional progress updates\n",
        "        eeg_data = self.remove_dc_offset(eeg_data)\n",
        "        eeg_data = self.bandpass_filter(eeg_data)\n",
        "        eeg_data = self.notch_filter(eeg_data)\n",
        "        eeg_data = self.remove_artifacts(eeg_data)\n",
        "        eeg_data = self.common_average_reference(eeg_data)\n",
        "        eeg_data = self.normalize(eeg_data)\n",
        "\n",
        "        return eeg_data\n",
        "\n",
        "    def process_batch(self, eeg_data_batch, verbose=False):\n",
        "        \"\"\"\n",
        "        Applies the complete preprocessing pipeline to a batch of EEG data\n",
        "\n",
        "        Args:\n",
        "            eeg_data_batch: Raw EEG data of shape (batch_size, n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Fully preprocessed batch of EEG data\n",
        "        \"\"\"\n",
        "        batch_size = eeg_data_batch.shape[0]\n",
        "        processed_batch = np.zeros_like(eeg_data_batch, dtype=np.float32)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Apply preprocessing pipeline to each sample in the batch\n",
        "            processed_batch[i] = self.process(eeg_data_batch[i], verbose)\n",
        "\n",
        "        return processed_batch\n",
        "\n",
        "class OptimizedFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts comprehensive features from preprocessed EEG signals\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fs=1000, feature_mode='full'):  # Changed default to 'full' for better features\n",
        "        \"\"\"\n",
        "        Initialize feature extractor\n",
        "\n",
        "        Args:\n",
        "            fs: Sampling frequency (default: 1000 Hz)\n",
        "            feature_mode: 'essential' for basic features, 'full' for advanced features\n",
        "        \"\"\"\n",
        "        self.fs = fs\n",
        "        self.feature_mode = feature_mode\n",
        "\n",
        "        # Define frequency bands\n",
        "        self.freq_bands = {\n",
        "            'delta': (0.5, 4),\n",
        "            'theta': (4, 8),\n",
        "            'alpha': (8, 13),\n",
        "            'beta': (13, 30),\n",
        "            'gamma': (30, 70)\n",
        "        }\n",
        "\n",
        "    def extract_time_domain_features(self, eeg_data, verbose=False):\n",
        "        \"\"\"\n",
        "        Extracts time domain features for each channel\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Preprocessed EEG data of shape (n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Time domain features\n",
        "        \"\"\"\n",
        "        n_channels = eeg_data.shape[0]\n",
        "        time_features = []\n",
        "\n",
        "        for ch in range(n_channels):\n",
        "            channel_data = eeg_data[ch, :]\n",
        "\n",
        "            # Basic statistics\n",
        "            mean = np.mean(channel_data)\n",
        "            variance = np.var(channel_data)\n",
        "            max_val = np.max(channel_data)\n",
        "            min_val = np.min(channel_data)\n",
        "            rms = np.sqrt(np.mean(np.square(channel_data)))\n",
        "\n",
        "            # Statistical measures\n",
        "            skewness = skew(channel_data)\n",
        "            kurt = kurtosis(channel_data)\n",
        "            peak_to_peak = max_val - min_val\n",
        "\n",
        "            # Zero crossings (signal oscillation rate)\n",
        "            zero_crossings = np.sum(np.diff(np.signbit(channel_data).astype(int)) != 0)\n",
        "\n",
        "            # Collect features\n",
        "            ch_features = [\n",
        "                mean, variance, max_val, min_val, rms,\n",
        "                skewness, kurt, peak_to_peak, zero_crossings\n",
        "            ]\n",
        "\n",
        "            time_features.extend(ch_features)\n",
        "\n",
        "        return np.array(time_features, dtype=np.float32)\n",
        "\n",
        "    def extract_frequency_domain_features(self, eeg_data, verbose=False):\n",
        "        \"\"\"\n",
        "        Extracts frequency domain features for each channel\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Preprocessed EEG data of shape (n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Frequency domain features\n",
        "        \"\"\"\n",
        "        n_channels = eeg_data.shape[0]\n",
        "        freq_features = []\n",
        "\n",
        "        for ch in range(n_channels):\n",
        "            channel_data = eeg_data[ch, :]\n",
        "\n",
        "            # Compute Power Spectral Density using Welch's method\n",
        "            freqs, psd = signal.welch(channel_data, fs=self.fs, nperseg=min(256, len(channel_data)))\n",
        "\n",
        "            # Total power\n",
        "            total_power = np.sum(psd)\n",
        "\n",
        "            # Band power\n",
        "            band_powers = {}\n",
        "            for band_name, (low_freq, high_freq) in self.freq_bands.items():\n",
        "                # Find indices corresponding to the frequency band\n",
        "                idx_band = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
        "                band_powers[band_name] = np.sum(psd[idx_band])\n",
        "\n",
        "            # Relative band power\n",
        "            rel_band_powers = {}\n",
        "            for band_name in self.freq_bands.keys():\n",
        "                if total_power > 0:\n",
        "                    rel_band_powers[band_name] = band_powers[band_name] / total_power\n",
        "                else:\n",
        "                    rel_band_powers[band_name] = 0\n",
        "\n",
        "            # Spectral entropy - measure of irregularity/complexity\n",
        "            psd_norm = psd / total_power if total_power > 0 else np.zeros_like(psd)\n",
        "            spec_entropy = -np.sum(psd_norm * np.log2(psd_norm + 1e-16))\n",
        "\n",
        "            # Collect features\n",
        "            ch_freq_features = [\n",
        "                total_power, spec_entropy,\n",
        "                band_powers['delta'], band_powers['theta'], band_powers['alpha'],\n",
        "                band_powers['beta'], band_powers['gamma'],\n",
        "                rel_band_powers['delta'], rel_band_powers['theta'], rel_band_powers['alpha'],\n",
        "                rel_band_powers['beta'], rel_band_powers['gamma']\n",
        "            ]\n",
        "\n",
        "            freq_features.extend(ch_freq_features)\n",
        "\n",
        "        return np.array(freq_features, dtype=np.float32)\n",
        "\n",
        "    def extract_advanced_features(self, eeg_data, verbose=False):\n",
        "        \"\"\"\n",
        "        Extracts advanced features (Hjorth parameters, connectivity measures)\n",
        "        Only used in 'full' feature mode\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Preprocessed EEG data of shape (n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Advanced features\n",
        "        \"\"\"\n",
        "        if self.feature_mode != 'full':\n",
        "            return np.array([])\n",
        "\n",
        "        n_channels = eeg_data.shape[0]\n",
        "        advanced_features = []\n",
        "\n",
        "        # Hjorth parameters for each channel\n",
        "        for ch in range(n_channels):\n",
        "            channel_data = eeg_data[ch, :]\n",
        "\n",
        "            # Activity (variance)\n",
        "            activity = np.var(channel_data)\n",
        "\n",
        "            # Mobility (standard deviation of first derivative / standard deviation of signal)\n",
        "            first_deriv = np.diff(channel_data, n=1)\n",
        "            first_deriv = np.append(first_deriv, first_deriv[-1])  # Padding to keep dimensions\n",
        "            mobility = np.std(first_deriv) / np.std(channel_data) if np.std(channel_data) > 0 else 0\n",
        "\n",
        "            # Complexity (mobility of first derivative / mobility of signal)\n",
        "            second_deriv = np.diff(first_deriv, n=1)\n",
        "            second_deriv = np.append(second_deriv, second_deriv[-1])  # Padding\n",
        "            complexity = (np.std(second_deriv) / np.std(first_deriv)) / mobility if mobility > 0 and np.std(first_deriv) > 0 else 0\n",
        "\n",
        "            advanced_features.extend([activity, mobility, complexity])\n",
        "\n",
        "        # Inter-channel correlation (connectivity measures)\n",
        "        if n_channels > 1:\n",
        "            correlations = []\n",
        "            for i in range(n_channels):\n",
        "                for j in range(i+1, n_channels):\n",
        "                    corr = np.corrcoef(eeg_data[i, :], eeg_data[j, :])[0, 1]\n",
        "                    correlations.append(corr)\n",
        "\n",
        "            # Add summary connectivity measures\n",
        "            advanced_features.append(np.mean(correlations))\n",
        "            advanced_features.append(np.max(correlations))\n",
        "\n",
        "        return np.array(advanced_features, dtype=np.float32)\n",
        "\n",
        "    def extract_features(self, eeg_data, verbose=False):\n",
        "        \"\"\"\n",
        "        Extracts all features from preprocessed EEG data\n",
        "\n",
        "        Args:\n",
        "            eeg_data: Preprocessed EEG data of shape (n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Feature vector\n",
        "        \"\"\"\n",
        "        # Extract features from different domains\n",
        "        time_features = self.extract_time_domain_features(eeg_data, verbose)\n",
        "        freq_features = self.extract_frequency_domain_features(eeg_data, verbose)\n",
        "        advanced_features = self.extract_advanced_features(eeg_data, verbose)\n",
        "\n",
        "        # Combine all features\n",
        "        all_features = np.concatenate([time_features, freq_features, advanced_features])\n",
        "\n",
        "        return all_features\n",
        "\n",
        "    def extract_features_batch(self, eeg_data_batch, verbose=False):\n",
        "        \"\"\"\n",
        "        Extract features from a batch of EEG data with improved vectorization\n",
        "\n",
        "        Args:\n",
        "            eeg_data_batch: Batch of preprocessed EEG data, shape (batch_size, n_channels, n_samples)\n",
        "            verbose: Whether to print progress updates\n",
        "\n",
        "        Returns:\n",
        "            Batch of feature vectors, shape (batch_size, n_features)\n",
        "        \"\"\"\n",
        "        batch_size = eeg_data_batch.shape[0]\n",
        "        all_features = []\n",
        "\n",
        "        # Process batches more efficiently\n",
        "        for i in range(batch_size):\n",
        "            eeg_data = eeg_data_batch[i]\n",
        "            features = self.extract_features(eeg_data, verbose)\n",
        "            all_features.append(features)\n",
        "\n",
        "        return np.stack(all_features)\n",
        "\n",
        "\n",
        "class MelSpectrogramGenerator:\n",
        "    \"\"\"\n",
        "    Generates 1-second fixed-length mel-spectrograms from text using Tacotron2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spec_length_seconds=1.0):  # Increased to 1.0 seconds from 0.5\n",
        "        \"\"\"\n",
        "        Initialize Tacotron2 model for text-to-mel-spectrogram conversion\n",
        "        \"\"\"\n",
        "        self.bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH\n",
        "        self.processor = self.bundle.get_text_processor()\n",
        "        self.tacotron2 = self.bundle.get_tacotron2()\n",
        "\n",
        "        # Standard parameters\n",
        "        self.sample_rate = 22050  # Hz\n",
        "        self.hop_length = 256     # Standard hop length for 22050Hz audio\n",
        "        self.n_mels = 80          # Standard number of mel channels\n",
        "        self.spec_length_seconds = spec_length_seconds\n",
        "\n",
        "        # Target length calculation based on spec_length_seconds\n",
        "        # frames = time * sample_rate / hop_length\n",
        "        self.target_length = int(self.spec_length_seconds * self.sample_rate / self.hop_length)\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tacotron2 = self.tacotron2.to(self.device)\n",
        "\n",
        "    def text_to_mel_spectrogram(self, text):\n",
        "        \"\"\"\n",
        "        Convert text to fixed-length mel-spectrogram using Tacotron2\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Mel-spectrogram tensor with shape [mel_channels, target_length]\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            # Encode text\n",
        "            inputs, input_lengths = self.processor(text)\n",
        "            inputs = inputs.to(self.device)\n",
        "            input_lengths = input_lengths.to(self.device)\n",
        "\n",
        "            # Generate spectrogram\n",
        "            spec, spec_lengths, _ = self.tacotron2.infer(inputs, input_lengths)\n",
        "\n",
        "        # Remove batch dimension if present\n",
        "        spec = spec.squeeze(0).cpu()\n",
        "\n",
        "        # Ensure the output is exactly target_length frames (1.0 seconds)\n",
        "        current_length = spec.shape[1]\n",
        "\n",
        "        if current_length > self.target_length:\n",
        "            # Truncate if longer than target\n",
        "            spec = spec[:, :self.target_length]\n",
        "        elif current_length < self.target_length:\n",
        "            # Pad with zeros if shorter than target\n",
        "            padding = torch.zeros(self.n_mels, self.target_length - current_length)\n",
        "            spec = torch.cat([spec, padding], dim=1)\n",
        "\n",
        "        return spec\n",
        "\n",
        "\n",
        "class EEGMelDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for EEG to mel-spectrogram mapping\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_path, label_file, transform=True, precompute=True, spec_length_seconds=1.0):\n",
        "        \"\"\"\n",
        "        Initialize dataset\n",
        "\n",
        "        Args:\n",
        "            data_path: Path to the .pth file containing EEG data\n",
        "            label_file: Path to the label text file\n",
        "            transform: Whether to preprocess and extract features from EEG\n",
        "            precompute: Whether to precompute and cache all features (slower startup, faster training)\n",
        "            spec_length_seconds: Length of the mel-spectrogram in seconds\n",
        "        \"\"\"\n",
        "        self.data = torch.load(data_path, weights_only=False)\n",
        "        self.label_dict = self._load_label_dict(label_file)\n",
        "        self.transform = transform\n",
        "        self.precompute = precompute\n",
        "        self.spec_length_seconds = spec_length_seconds\n",
        "\n",
        "        # Initialize preprocessor and feature extractor if transform is True\n",
        "        if transform:\n",
        "            self.preprocessor = EEGPreprocessor()\n",
        "            self.feature_extractor = OptimizedFeatureExtractor(feature_mode='full')\n",
        "\n",
        "        # Initialize mel-spectrogram generator with fixed output length\n",
        "        self.mel_generator = MelSpectrogramGenerator(spec_length_seconds=spec_length_seconds)\n",
        "\n",
        "        # Find unique label IDs actually used in the dataset\n",
        "        unique_label_ids = set()\n",
        "        for sample in self.data['dataset']:\n",
        "            label_id = sample['label']\n",
        "            unique_label_ids.add(label_id)\n",
        "\n",
        "        # Generate all mel-spectrograms in advance for the unique labels\n",
        "        self.mel_spectrograms = {}\n",
        "        for label_id in unique_label_ids:\n",
        "            if label_id in self.label_dict:\n",
        "                text = self.label_dict[label_id]\n",
        "                self.mel_spectrograms[label_id] = self.mel_generator.text_to_mel_spectrogram(text)\n",
        "\n",
        "        # Precompute all EEG features if requested (to avoid recomputing during training)\n",
        "        self.precomputed_features = {}\n",
        "        if precompute and transform:\n",
        "            total_samples = len(self.data['dataset'])\n",
        "            print(f\"\\nPrecomputing EEG features for all {total_samples} samples...\")\n",
        "\n",
        "            # Process in batches for better efficiency\n",
        "            batch_size = 32  # Adjust based on memory constraints\n",
        "            num_batches = (total_samples + batch_size - 1) // batch_size\n",
        "\n",
        "            with tqdm(total=total_samples, desc=\"Extracting features\") as pbar:\n",
        "                for batch_idx in range(num_batches):\n",
        "                    start_idx = batch_idx * batch_size\n",
        "                    end_idx = min(start_idx + batch_size, total_samples)\n",
        "                    current_batch_size = end_idx - start_idx\n",
        "\n",
        "                    # Collect batch of EEG data\n",
        "                    eeg_batch = []\n",
        "                    for i in range(start_idx, end_idx):\n",
        "                        eeg_batch.append(self.data['dataset'][i]['eeg_data'])\n",
        "\n",
        "                    # Convert list to numpy array for batch processing\n",
        "                    eeg_batch_array = np.array(eeg_batch)\n",
        "\n",
        "                    # Process entire batch at once - first preprocessing\n",
        "                    processed_batch = self.preprocessor.process_batch(eeg_batch_array, verbose=False)\n",
        "\n",
        "                    # Then extract features in batch mode\n",
        "                    feature_batch = self.feature_extractor.extract_features_batch(processed_batch, verbose=False)\n",
        "\n",
        "                    # Store the features\n",
        "                    for i, sample_idx in enumerate(range(start_idx, end_idx)):\n",
        "                        self.precomputed_features[sample_idx] = torch.tensor(feature_batch[i], dtype=torch.float32)\n",
        "\n",
        "                    pbar.update(current_batch_size)\n",
        "\n",
        "                    # Clear memory periodically\n",
        "                    if batch_idx % 5 == 0 and torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Successfully precomputed features for all {total_samples} samples\")\n",
        "\n",
        "    def _load_label_dict(self, label_file):\n",
        "        \"\"\"\n",
        "        Load label dictionary from label file\n",
        "\n",
        "        Args:\n",
        "            label_file: Path to the label text file\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping label_id to corresponding text\n",
        "        \"\"\"\n",
        "        label_dict = {}\n",
        "        with open(label_file, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(maxsplit=1)\n",
        "                if len(parts) == 2:\n",
        "                    label_id, text = parts\n",
        "                    label_dict[label_id] = text\n",
        "\n",
        "        return label_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of samples in the dataset\n",
        "        \"\"\"\n",
        "        return len(self.data['dataset'])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Return a sample from the dataset\n",
        "\n",
        "        Args:\n",
        "            index: Sample index\n",
        "\n",
        "        Returns:\n",
        "            EEG features and corresponding mel-spectrogram\n",
        "        \"\"\"\n",
        "        # Get EEG data and label\n",
        "        eeg_data = self.data['dataset'][index]['eeg_data']\n",
        "        label_id = self.data['dataset'][index]['label']\n",
        "\n",
        "        # Use precomputed features if available, otherwise process on the fly\n",
        "        if self.precompute and index in self.precomputed_features:\n",
        "            eeg_features = self.precomputed_features[index]\n",
        "        elif self.transform:\n",
        "            # Preprocess and extract features\n",
        "            eeg_data = self.preprocessor.process(eeg_data, verbose=False)\n",
        "            eeg_features = self.feature_extractor.extract_features(eeg_data, verbose=False)\n",
        "            eeg_features = torch.tensor(eeg_features, dtype=torch.float32)\n",
        "        else:\n",
        "            eeg_features = eeg_data.flatten()\n",
        "\n",
        "        # Get corresponding mel-spectrogram\n",
        "        mel_spec = self.mel_spectrograms[label_id]\n",
        "\n",
        "        return eeg_features, mel_spec\n",
        "\n",
        "\n",
        "# Implementing the requested MultiResolutionSpectralLoss\n",
        "class MultiResolutionSpectralLoss(nn.Module):\n",
        "    def __init__(self,\n",
        "                 fft_sizes=[1024, 2048, 512],\n",
        "                 hop_sizes=[120, 240, 50],\n",
        "                 win_lengths=[600, 1200, 240],\n",
        "                 device='cuda'):\n",
        "        super(MultiResolutionSpectralLoss, self).__init__()\n",
        "        self.fft_sizes = fft_sizes\n",
        "        self.hop_sizes = hop_sizes\n",
        "        self.win_lengths = win_lengths\n",
        "        self.device = device\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "\n",
        "    def forward(self, pred_spec, target_spec):\n",
        "        # Basic L1 loss between predicted and target spectrograms\n",
        "        l1_loss = self.l1_loss(pred_spec, target_spec)\n",
        "\n",
        "        # Spectral convergence loss (normalized Frobenius norm of difference)\n",
        "        # Prevents NaN by adding a small epsilon to the denominator\n",
        "        epsilon = 1e-8\n",
        "        sc_loss = torch.norm(target_spec - pred_spec, p='fro') / (torch.norm(target_spec, p='fro') + epsilon)\n",
        "\n",
        "        # Combine losses\n",
        "        total_loss = l1_loss + sc_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "# Improved neural network architecture with residual connections and deeper layers\n",
        "class ImprovedEEGToMelModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved neural network model for predicting mel-spectrograms from EEG features\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, mel_channels):\n",
        "        \"\"\"\n",
        "        Initialize model with improved architecture\n",
        "\n",
        "        Args:\n",
        "            input_dim: Dimension of input EEG features\n",
        "            hidden_dim: Dimension of hidden layers\n",
        "            output_dim: Dimension of output sequence length (time frames)\n",
        "            mel_channels: Number of mel channels in the output spectrogram\n",
        "        \"\"\"\n",
        "        super(ImprovedEEGToMelModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.mel_channels = mel_channels\n",
        "\n",
        "        # Initial normalization\n",
        "        self.input_norm = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # First block with residual connection\n",
        "        self.block1_fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.block1_bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.block1_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.block1_bn2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.block1_residual = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Second block with residual connection\n",
        "        self.block2_fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.block2_bn1 = nn.BatchNorm1d(hidden_dim // 2)\n",
        "        self.block2_fc2 = nn.Linear(hidden_dim // 2, hidden_dim // 2)\n",
        "        self.block2_bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
        "        self.block2_residual = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "\n",
        "        # Third block\n",
        "        self.block3_fc1 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
        "        self.block3_bn1 = nn.BatchNorm1d(hidden_dim // 4)\n",
        "        self.block3_fc2 = nn.Linear(hidden_dim // 4, hidden_dim // 4)\n",
        "        self.block3_bn2 = nn.BatchNorm1d(hidden_dim // 4)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention_query = nn.Linear(hidden_dim // 4, hidden_dim // 8)\n",
        "        self.attention_key = nn.Linear(hidden_dim // 4, hidden_dim // 8)\n",
        "        self.attention_value = nn.Linear(hidden_dim // 4, hidden_dim // 4)\n",
        "\n",
        "        # Final processing\n",
        "        self.fc_final = nn.Linear(hidden_dim // 4, hidden_dim // 4)\n",
        "        self.bn_final = nn.BatchNorm1d(hidden_dim // 4)\n",
        "\n",
        "        # Output projection\n",
        "        self.output = nn.Linear(hidden_dim // 4, output_dim * mel_channels)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass with residual connections and attention\n",
        "\n",
        "        Args:\n",
        "            x: Input EEG features\n",
        "\n",
        "        Returns:\n",
        "            Predicted mel-spectrogram\n",
        "        \"\"\"\n",
        "        # Input normalization\n",
        "        x_norm = self.input_norm(x)\n",
        "\n",
        "        # Block 1 with residual connection\n",
        "        residual = self.block1_residual(x_norm)\n",
        "        out = self.leaky_relu(self.block1_bn1(self.block1_fc1(x_norm)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.leaky_relu(self.block1_bn2(self.block1_fc2(out)))\n",
        "        out = out + residual  # Residual connection\n",
        "\n",
        "        # Block 2 with residual connection\n",
        "        residual = self.block2_residual(out)\n",
        "        out = self.leaky_relu(self.block2_bn1(self.block2_fc1(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.leaky_relu(self.block2_bn2(self.block2_fc2(out)))\n",
        "        out = out + residual  # Residual connection\n",
        "\n",
        "        # Block 3\n",
        "        out = self.leaky_relu(self.block3_bn1(self.block3_fc1(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = self.leaky_relu(self.block3_bn2(self.block3_fc2(out)))\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        q = self.attention_query(out)  # [batch_size, hidden_dim//8]\n",
        "        k = self.attention_key(out)    # [batch_size, hidden_dim//8]\n",
        "        v = self.attention_value(out)  # [batch_size, hidden_dim//4]\n",
        "\n",
        "        # Compute attention scores (simplified self-attention for 1D features)\n",
        "        attn_scores = torch.matmul(q.unsqueeze(1), k.unsqueeze(2)) / (self.hidden_dim ** 0.5)  # [batch_size, 1, 1]\n",
        "        attn_weights = torch.softmax(attn_scores, dim=2)  # [batch_size, 1, 1]\n",
        "\n",
        "        # Apply attention\n",
        "        context = attn_weights * v.unsqueeze(1)  # [batch_size, 1, hidden_dim//4]\n",
        "        context = context.squeeze(1)  # [batch_size, hidden_dim//4]\n",
        "\n",
        "        # Final processing\n",
        "        out = out + context  # Add attention context (residual)\n",
        "        out = self.leaky_relu(self.bn_final(self.fc_final(out)))\n",
        "\n",
        "        # Output projection\n",
        "        out = self.output(out)\n",
        "\n",
        "        # Reshape to spectrogram dimensions [batch, mel_channels, time]\n",
        "        out = out.view(-1, self.mel_channels, self.output_dim)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Update your train function with mixed precision\n",
        "def train(model, train_loader, val_loader, device, num_epochs=100, lr=0.001):\n",
        "    \"\"\"\n",
        "    Train the model with improved loss function, learning strategy, and mixed precision\n",
        "    \"\"\"\n",
        "    # Custom spectral loss for mel-spectrograms\n",
        "    criterion = MultiResolutionSpectralLoss(device=device)\n",
        "\n",
        "    # Using AdamW optimizer with weight decay for regularization\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "    # Learning rate scheduler to reduce LR on plateau\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Initialize gradient scaler for mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    # Track best validation loss\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    # For early stopping\n",
        "    early_stop_counter = 0\n",
        "    early_stop_patience = 10\n",
        "\n",
        "    # Training history\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Optimize CUDA operations\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Progress bar for training\n",
        "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
        "            for i, (eeg_features, target_specs) in enumerate(pbar):\n",
        "                # Move data to device\n",
        "                eeg_features = eeg_features.to(device, non_blocking=True)\n",
        "                target_specs = target_specs.to(device, non_blocking=True)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad(set_to_none=True)  # More efficient than standard zero_grad()\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast():\n",
        "                    outputs = model(eeg_features)\n",
        "                    # Compute loss\n",
        "                    loss = criterion(outputs, target_specs)\n",
        "\n",
        "                # Check for NaN loss\n",
        "                if torch.isnan(loss).any():\n",
        "                    print(f\"WARNING: NaN loss detected! Skipping batch {i}\")\n",
        "                    continue\n",
        "\n",
        "                # Backward pass with scaled gradients\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Gradient clipping on scaled gradients\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Update weights with scaled gradients\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "                # Update running loss (using item() to avoid memory leaks)\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix(loss=loss.item(), lr=optimizer.param_groups[0]['lr'])\n",
        "\n",
        "                # Optional: Free up memory explicitly (can help on limited GPU memory)\n",
        "                if (i+1) % 10 == 0:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate average training loss\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for eeg_features, target_specs in val_loader:\n",
        "                eeg_features = eeg_features.to(device, non_blocking=True)\n",
        "                target_specs = target_specs.to(device, non_blocking=True)\n",
        "\n",
        "                # Use mixed precision for validation too\n",
        "                with autocast():\n",
        "                    outputs = model(eeg_features)\n",
        "                    loss = criterion(outputs, target_specs)\n",
        "\n",
        "                # Skip NaN losses during validation too\n",
        "                if not torch.isnan(loss).any():\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            early_stop_counter = 0\n",
        "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "            print(f\"Validation loss did not improve. Early stopping counter: {early_stop_counter}/{early_stop_patience}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stop_counter >= early_stop_patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Explicitly clear memory at the end of epoch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig('training_loss.png')\n",
        "    plt.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, device, num_samples=5):\n",
        "    \"\"\"\n",
        "    Evaluate model and visualize predicted spectrograms compared to ground truth\n",
        "\n",
        "    Args:\n",
        "        model: Trained EEGToMelModel\n",
        "        test_loader: DataLoader for test data\n",
        "        device: Device to evaluate on (cuda/cpu)\n",
        "        num_samples: Number of samples to visualize\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    test_samples = []\n",
        "\n",
        "    # Get some test samples\n",
        "    with torch.no_grad():\n",
        "        for eeg_features, target_specs in test_loader:\n",
        "            batch_size = eeg_features.shape[0]\n",
        "            for i in range(min(batch_size, num_samples - len(test_samples))):\n",
        "                if len(test_samples) >= num_samples:\n",
        "                    break\n",
        "\n",
        "                single_eeg = eeg_features[i:i+1].to(device)\n",
        "                single_target = target_specs[i:i+1].to(device)\n",
        "\n",
        "                # Generate prediction\n",
        "                pred_spec = model(single_eeg)\n",
        "\n",
        "                test_samples.append({\n",
        "                    'eeg': single_eeg.cpu(),\n",
        "                    'target': single_target.cpu(),\n",
        "                    'pred': pred_spec.cpu()\n",
        "                })\n",
        "\n",
        "            if len(test_samples) >= num_samples:\n",
        "                break\n",
        "\n",
        "    # Create directory for results\n",
        "    os.makedirs('evaluation_results', exist_ok=True)\n",
        "\n",
        "    # Visualize spectrograms\n",
        "    plt.figure(figsize=(15, num_samples*5))\n",
        "\n",
        "    for i, sample in enumerate(test_samples):\n",
        "        target_spec = sample['target'].squeeze().numpy()\n",
        "        pred_spec = sample['pred'].squeeze().numpy()\n",
        "\n",
        "        # Plot target spectrogram\n",
        "        plt.subplot(num_samples, 2, i*2+1)\n",
        "        plt.imshow(target_spec, aspect='auto', origin='lower')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Sample {i+1}: Target Mel-Spectrogram')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Plot predicted spectrogram\n",
        "        plt.subplot(num_samples, 2, i*2+2)\n",
        "        plt.imshow(pred_spec, aspect='auto', origin='lower')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Sample {i+1}: Predicted Mel-Spectrogram')\n",
        "        plt.tight_layout()\n",
        "\n",
        "    plt.savefig('evaluation_results/spectrogram_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Evaluation complete. Visualization saved to 'evaluation_results/spectrogram_comparison.png'\")\n",
        "\n",
        "\n",
        "# Update the main function to optimize data loading\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to train and evaluate the model with optimized settings for Kaggle P100\n",
        "    \"\"\"\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Parameters\n",
        "    data_path = '/kaggle/working/session_1.pth'  # Path to EEG data\n",
        "    label_file = '/kaggle/input/labels-file/labels_txt.txt'    # Path to label file\n",
        "    batch_size = 64  # Increased batch size for better GPU utilization\n",
        "    num_epochs = 100\n",
        "    spec_length_seconds = 1.0\n",
        "\n",
        "    # Enable GPU memory optimization\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # Use deterministic algorithms when needed\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # Select device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Set optimal pin memory and worker settings for Kaggle\n",
        "    pin_memory = True if torch.cuda.is_available() else False\n",
        "    num_workers = 2  # Optimal for Kaggle notebooks (don't use too many)\n",
        "\n",
        "    # Create dataset with memory optimization\n",
        "    print(\"Loading and preprocessing dataset...\")\n",
        "    dataset = EEGMelDataset(\n",
        "        data_path=data_path,\n",
        "        label_file=label_file,\n",
        "        transform=True,\n",
        "        precompute=True,  # Still precomputing features as it's beneficial despite startup cost\n",
        "        spec_length_seconds=spec_length_seconds\n",
        "    )\n",
        "\n",
        "    # Get a sample to determine input and output dimensions\n",
        "    sample_eeg_features, sample_mel_spec = dataset[0]\n",
        "\n",
        "    input_dim = sample_eeg_features.shape[0]\n",
        "    mel_channels = sample_mel_spec.shape[0]\n",
        "    output_dim = sample_mel_spec.shape[1]\n",
        "\n",
        "    print(f\"Input feature dimension: {input_dim}\")\n",
        "    print(f\"Output mel-spectrogram shape: {mel_channels}x{output_dim}\")\n",
        "\n",
        "    # Split dataset with reproducible generator\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    val_size = int(0.15 * len(dataset))\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "    # Note: using a fixed random generator for splitting to ensure reproducibility\n",
        "    generator = torch.Generator().manual_seed(42)\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Create optimized data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        prefetch_factor=2,  # Pre-fetch 2 batches per worker\n",
        "        persistent_workers=True  # Keep workers alive between epochs\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=False,\n",
        "        num_workers=1,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    print(\"Initializing model...\")\n",
        "    hidden_dim = 512\n",
        "    model = ImprovedEEGToMelModel(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        output_dim=output_dim,\n",
        "        mel_channels=mel_channels\n",
        "    ).to(device)\n",
        "\n",
        "    # Display model architecture and parameter count\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model created with {total_params:,} parameters\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    model = train(model, train_loader, val_loader, device, num_epochs=num_epochs)\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), 'eeg_to_mel_model.pth')\n",
        "    print(\"Model saved to 'eeg_to_mel_model.pth'\")\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    evaluate_model(model, test_loader, device)\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VSZfkzD1zlYe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfF3UVwW97z3",
        "outputId": "4eb46dcf-2cdd-4907-88fa-7e7ee813ad05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "BYdZuI1ozlYi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import your model classes and preprocessing modules\n",
        "# Make sure these imports match your actual implementation\n",
        "# from model import ImprovedEEGToMelModel\n",
        "# from preprocessing import EEGPreprocessor, OptimizedFeatureExtractor\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    \"\"\"\n",
        "    Load the saved EEG to mel-spectrogram model\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model checkpoint\n",
        "        device: Device to load the model on\n",
        "\n",
        "    Returns:\n",
        "        Loaded model and model parameters\n",
        "    \"\"\"\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "    # If the checkpoint is just the state dict, we need to determine the model parameters\n",
        "    if isinstance(checkpoint, dict) and 'input_dim' in checkpoint:\n",
        "        # Structured checkpoint with parameters\n",
        "        input_dim = checkpoint['input_dim']\n",
        "        hidden_dim = checkpoint['hidden_dim']\n",
        "        output_dim = checkpoint['output_dim']\n",
        "        mel_channels = checkpoint['mel_channels']\n",
        "        model_state_dict = checkpoint['model_state_dict']\n",
        "    else:\n",
        "        # Just the state dict - we need to extract dimensions from the keys\n",
        "        model_state_dict = checkpoint\n",
        "\n",
        "        # Try to determine dimensions from the model state dict keys\n",
        "        input_dim = model_state_dict['block1_fc1.weight'].size(1)\n",
        "        hidden_dim = model_state_dict['block1_fc1.weight'].size(0)\n",
        "        output_shape = model_state_dict['output.weight'].size()\n",
        "        mel_channels = 80  # Standard number from the MelSpectrogramGenerator class\n",
        "        output_dim = output_shape[0] // mel_channels\n",
        "\n",
        "    # Initialize model\n",
        "    model = ImprovedEEGToMelModel(input_dim, hidden_dim, output_dim, mel_channels)\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    # model=torch.load(model_path, map_location=device,weights_only=False)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Model loaded with:\")\n",
        "    print(f\"- Input dimension: {input_dim}\")\n",
        "    print(f\"- Hidden dimension: {hidden_dim}\")\n",
        "    print(f\"- Output time dimension: {output_dim} frames\")\n",
        "    print(f\"- Mel channels: {mel_channels}\")\n",
        "\n",
        "    return model, output_dim, mel_channels\n",
        "\n",
        "def process_eeg_sample(eeg_data, preprocessor, feature_extractor):\n",
        "    \"\"\"\n",
        "    Process an EEG sample to extract features\n",
        "\n",
        "    Args:\n",
        "        eeg_data: Raw EEG data\n",
        "        preprocessor: EEG preprocessor\n",
        "        feature_extractor: Feature extractor\n",
        "\n",
        "    Returns:\n",
        "        Extracted features as a tensor\n",
        "    \"\"\"\n",
        "    # Preprocess and extract features\n",
        "    processed_eeg = preprocessor.process(eeg_data, verbose=False)\n",
        "    features = feature_extractor.extract_features(processed_eeg, verbose=False)\n",
        "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
        "\n",
        "    return features_tensor\n",
        "\n",
        "def predict_mel_spectrogram(model, eeg_features, device):\n",
        "    \"\"\"\n",
        "    Predict mel-spectrogram from EEG features\n",
        "\n",
        "    Args:\n",
        "        model: Trained EEG to mel-spectrogram model\n",
        "        eeg_features: EEG features\n",
        "        device: Device to run inference on\n",
        "\n",
        "    Returns:\n",
        "        Predicted mel-spectrogram\n",
        "    \"\"\"\n",
        "    # Prepare input\n",
        "    eeg_features = eeg_features.unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate prediction\n",
        "    with torch.no_grad():\n",
        "        predicted_mel = model(eeg_features).cpu().squeeze(0)\n",
        "\n",
        "    return predicted_mel\n",
        "\n",
        "def convert_mel_to_audio(mel_spectrogram):\n",
        "    \"\"\"\n",
        "    Convert mel-spectrogram to audio using WaveRNN vocoder\n",
        "\n",
        "    Args:\n",
        "        mel_spectrogram: Predicted mel-spectrogram\n",
        "\n",
        "    Returns:\n",
        "        Audio waveform and sample rate (as integer)\n",
        "    \"\"\"\n",
        "    # Get the WaveRNN vocoder from torchaudio\n",
        "    bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH\n",
        "    vocoder = bundle.get_vocoder()\n",
        "\n",
        "    # Determine the device\n",
        "    device = next(vocoder.parameters()).device\n",
        "\n",
        "    # Ensure the mel spectrogram has the correct format [batch, mel_channels, time]\n",
        "    # First make sure it's a 3D tensor with batch dimension\n",
        "    if mel_spectrogram.dim() == 2:\n",
        "        mel_spectrogram = mel_spectrogram.unsqueeze(0)\n",
        "\n",
        "    # Move to the device\n",
        "    mel_spectrogram = mel_spectrogram.to(device)\n",
        "\n",
        "    # Set lengths tensor (assuming full length)\n",
        "    lengths = torch.tensor([mel_spectrogram.shape[2]], device=device)\n",
        "\n",
        "    # Convert spectrogram to waveform\n",
        "    with torch.no_grad():\n",
        "        waveforms, sample_rate = vocoder(mel_spectrogram, lengths)\n",
        "\n",
        "    # Convert sample_rate from tensor to integer\n",
        "    if torch.is_tensor(sample_rate):\n",
        "        sample_rate = sample_rate.item()\n",
        "\n",
        "    return waveforms.cpu(), sample_rate\n",
        "\n",
        "def load_eeg_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Load EEG data from a .pth file\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the .pth file containing EEG data\n",
        "\n",
        "    Returns:\n",
        "        EEG data from the file\n",
        "    \"\"\"\n",
        "    data = torch.load(file_path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # Handle different data formats\n",
        "    if isinstance(data, dict):\n",
        "        # If the data is stored in a dictionary\n",
        "        if 'eeg_data' in data:\n",
        "            return data['eeg_data']\n",
        "        elif 'dataset' in data and len(data['dataset']) > 0:\n",
        "            return data['dataset'][0]['eeg_data']\n",
        "    elif isinstance(data, torch.Tensor):\n",
        "        # If the data is directly a tensor\n",
        "        return data\n",
        "\n",
        "    raise ValueError(f\"Could not extract EEG data from file: {file_path}\")\n",
        "\n",
        "def generate_submission(model_path, test_files_dir, output_dir='submission'):\n",
        "    \"\"\"\n",
        "    Generate submission files for the competition\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the saved model\n",
        "        test_files_dir: Directory containing the test .pth files\n",
        "        output_dir: Directory to save outputs\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    model, output_dim, mel_channels = load_model(model_path, device)\n",
        "\n",
        "    # Find all test files\n",
        "    test_files = glob.glob(os.path.join(test_files_dir, \"*.pth\"))\n",
        "    print(f\"Found {len(test_files)} test files\")\n",
        "\n",
        "    if len(test_files) != 12:\n",
        "        print(f\"Warning: Expected 12 test files, but found {len(test_files)}\")\n",
        "\n",
        "    # Initialize preprocessor and feature extractor\n",
        "    preprocessor = EEGPreprocessor()\n",
        "    feature_extractor = OptimizedFeatureExtractor(feature_mode='full')\n",
        "\n",
        "    # Process each test file\n",
        "    for idx, test_file in enumerate(tqdm(test_files, desc=\"Processing test files\")):\n",
        "        # Extract file name without extension\n",
        "        file_name = os.path.splitext(os.path.basename(test_file))[0]\n",
        "        print(f\"\\nProcessing test file {idx + 1}/{len(test_files)}: {file_name}\")\n",
        "\n",
        "        try:\n",
        "            # Load EEG data\n",
        "            eeg_data = load_eeg_from_file(test_file)\n",
        "            print(f\"- EEG data shape: {eeg_data.shape}\")\n",
        "\n",
        "            # Process EEG sample\n",
        "            print(\"- Extracting features...\")\n",
        "            eeg_features = process_eeg_sample(eeg_data, preprocessor, feature_extractor)\n",
        "\n",
        "            # Predict mel-spectrogram\n",
        "            print(\"- Predicting mel-spectrogram...\")\n",
        "            predicted_mel = predict_mel_spectrogram(model, eeg_features, device)\n",
        "\n",
        "            # Save the predicted mel-spectrogram as .pth file\n",
        "            mel_path = os.path.join(output_dir, f\"{file_name}_mel.pth\")\n",
        "            torch.save(predicted_mel, mel_path)\n",
        "            print(f\"- Mel spectrogram saved to {mel_path}\")\n",
        "\n",
        "            # Save mel-spectrogram visualization\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(predicted_mel.numpy(), aspect='auto', origin='lower')\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title(f\"Test File: {file_name} - Predicted Mel Spectrogram\")\n",
        "            plt.xlabel(\"Time\")\n",
        "            plt.ylabel(\"Mel Channels\")\n",
        "            plt.tight_layout()\n",
        "            mel_plot_path = os.path.join(output_dir, f\"{file_name}_mel.png\")\n",
        "            plt.savefig(mel_plot_path)\n",
        "            plt.close()\n",
        "            print(f\"- Mel spectrogram visualization saved to {mel_plot_path}\")\n",
        "\n",
        "            # Convert predicted mel to audio\n",
        "            print(\"- Converting predicted mel to audio...\")\n",
        "            waveform, sample_rate = convert_mel_to_audio(predicted_mel)\n",
        "\n",
        "            # Save audio file\n",
        "            audio_path = os.path.join(output_dir, f\"{file_name}_audio.wav\")\n",
        "            torchaudio.save(audio_path, waveform, sample_rate)\n",
        "            print(f\"- Audio saved to {audio_path}\")\n",
        "\n",
        "            # Plot waveform\n",
        "            plt.figure(figsize=(10, 3))\n",
        "            plt.plot(waveform.squeeze().numpy())\n",
        "            plt.title(f\"Test File: {file_name} - Predicted Waveform\")\n",
        "            plt.xlabel(\"Time (samples)\")\n",
        "            plt.ylabel(\"Amplitude\")\n",
        "            plt.tight_layout()\n",
        "            wave_plot_path = os.path.join(output_dir, f\"{file_name}_waveform.png\")\n",
        "            plt.savefig(wave_plot_path)\n",
        "            plt.close()\n",
        "            print(f\"- Waveform visualization saved to {wave_plot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"- Error processing test file {file_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    print(\"\\nSubmission generation complete!\")\n",
        "    print(f\"All files saved in {output_dir}\")\n",
        "\n",
        "    # Create a list of files to be included in the zip for submission\n",
        "    submission_files = []\n",
        "    for file_name in os.listdir(output_dir):\n",
        "        if file_name.endswith(\"_mel.pth\") or file_name.endswith(\"_audio.wav\"):\n",
        "            submission_files.append(os.path.join(output_dir, file_name))\n",
        "\n",
        "    print(f\"\\nFiles to include in submission zip ({len(submission_files)}):\")\n",
        "    for file in submission_files:\n",
        "        print(f\"- {os.path.basename(file)}\")\n",
        "\n",
        "    print(\"\\nPlease zip these files and upload them to the competition website.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update these paths according to your environment\n",
        "    model_path = 'eeg_to_mel_model.pth'  # Path to your trained model\n",
        "    test_files_dir = '/content/drive/MyDrive/TEST'           # Directory containing 12 test .pth files\n",
        "    output_dir = './submission'                   # Output directory for submission files\n",
        "\n",
        "    generate_submission(model_path, test_files_dir, output_dir)"
      ],
      "metadata": {
        "trusted": true,
        "id": "iCqcaZJDzlYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896156e6-c572-4ce0-892b-27fada6ff9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Model loaded with:\n",
            "- Input dimension: 1490\n",
            "- Hidden dimension: 512\n",
            "- Output time dimension: 86 frames\n",
            "- Mel channels: 80\n",
            "Found 12 test files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:   0%|          | 0/12 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing test file 1/12: Copy of sample_2\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_2_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_2_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wavernn_10k_epochs_8bits_ljspeech.pth\" to /root/.cache/torch/hub/checkpoints/wavernn_10k_epochs_8bits_ljspeech.pth\n",
            "\n",
            "  0%|          | 0.00/16.7M [00:00<?, ?B/s]\u001b[A\n",
            "  4%|▍         | 640k/16.7M [00:00<00:02, 6.43MB/s]\u001b[A\n",
            "100%|██████████| 16.7M/16.7M [00:00<00:00, 66.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_2_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:   8%|▊         | 1/12 [01:21<14:52, 81.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Waveform visualization saved to ./submission/Copy of sample_2_waveform.png\n",
            "\n",
            "Processing test file 2/12: Copy of sample_1\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_1_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_1_mel.png\n",
            "- Converting predicted mel to audio...\n",
            "- Audio saved to ./submission/Copy of sample_1_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  17%|█▋        | 2/12 [02:36<12:58, 77.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Waveform visualization saved to ./submission/Copy of sample_1_waveform.png\n",
            "\n",
            "Processing test file 3/12: Copy of sample_7\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_7_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_7_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  25%|██▌       | 3/12 [03:54<11:40, 77.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_7_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_7_waveform.png\n",
            "\n",
            "Processing test file 4/12: Copy of sample_6\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_6_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_6_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  33%|███▎      | 4/12 [05:10<10:16, 77.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_6_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_6_waveform.png\n",
            "\n",
            "Processing test file 5/12: Copy of sample_5\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_5_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_5_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  42%|████▏     | 5/12 [06:25<08:55, 76.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_5_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_5_waveform.png\n",
            "\n",
            "Processing test file 6/12: Copy of sample_4\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_4_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_4_mel.png\n",
            "- Converting predicted mel to audio...\n",
            "- Audio saved to ./submission/Copy of sample_4_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  50%|█████     | 6/12 [07:41<07:36, 76.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Waveform visualization saved to ./submission/Copy of sample_4_waveform.png\n",
            "\n",
            "Processing test file 7/12: Copy of sample_3\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_3_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_3_mel.png\n",
            "- Converting predicted mel to audio...\n",
            "- Audio saved to ./submission/Copy of sample_3_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  58%|█████▊    | 7/12 [09:11<06:42, 80.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Waveform visualization saved to ./submission/Copy of sample_3_waveform.png\n",
            "\n",
            "Processing test file 8/12: Copy of sample_12\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_12_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_12_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  67%|██████▋   | 8/12 [10:26<05:16, 79.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_12_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_12_waveform.png\n",
            "\n",
            "Processing test file 9/12: Copy of sample_11\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_11_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_11_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  75%|███████▌  | 9/12 [11:42<03:53, 77.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_11_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_11_waveform.png\n",
            "\n",
            "Processing test file 10/12: Copy of sample_10\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_10_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_10_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  83%|████████▎ | 10/12 [12:57<02:34, 77.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_10_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_10_waveform.png\n",
            "\n",
            "Processing test file 11/12: Copy of sample_9\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_9_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_9_mel.png\n",
            "- Converting predicted mel to audio...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing test files:  92%|█████████▏| 11/12 [14:13<01:16, 76.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Audio saved to ./submission/Copy of sample_9_audio.wav\n",
            "- Waveform visualization saved to ./submission/Copy of sample_9_waveform.png\n",
            "\n",
            "Processing test file 12/12: Copy of sample_8\n",
            "- EEG data shape: torch.Size([62, 501])\n",
            "- Extracting features...\n",
            "- Predicting mel-spectrogram...\n",
            "- Mel spectrogram saved to ./submission/Copy of sample_8_mel.pth\n",
            "- Mel spectrogram visualization saved to ./submission/Copy of sample_8_mel.png\n",
            "- Converting predicted mel to audio...\n",
            "- Audio saved to ./submission/Copy of sample_8_audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing test files: 100%|██████████| 12/12 [15:29<00:00, 77.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Waveform visualization saved to ./submission/Copy of sample_8_waveform.png\n",
            "\n",
            "Submission generation complete!\n",
            "All files saved in ./submission\n",
            "\n",
            "Files to include in submission zip (24):\n",
            "- Copy of sample_9_mel.pth\n",
            "- Copy of sample_9_audio.wav\n",
            "- Copy of sample_3_audio.wav\n",
            "- Copy of sample_2_mel.pth\n",
            "- Copy of sample_6_mel.pth\n",
            "- Copy of sample_12_audio.wav\n",
            "- Copy of sample_3_mel.pth\n",
            "- Copy of sample_8_audio.wav\n",
            "- Copy of sample_11_audio.wav\n",
            "- Copy of sample_1_audio.wav\n",
            "- Copy of sample_5_audio.wav\n",
            "- Copy of sample_6_audio.wav\n",
            "- Copy of sample_7_mel.pth\n",
            "- Copy of sample_8_mel.pth\n",
            "- Copy of sample_12_mel.pth\n",
            "- Copy of sample_11_mel.pth\n",
            "- Copy of sample_7_audio.wav\n",
            "- Copy of sample_5_mel.pth\n",
            "- Copy of sample_2_audio.wav\n",
            "- Copy of sample_4_mel.pth\n",
            "- Copy of sample_1_mel.pth\n",
            "- Copy of sample_10_mel.pth\n",
            "- Copy of sample_10_audio.wav\n",
            "- Copy of sample_4_audio.wav\n",
            "\n",
            "Please zip these files and upload them to the competition website.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Using Hi-FI GAN#\n",
        "#####cause it was hard to accomodate as per stencil in my code , so given here#####"
      ],
      "metadata": {
        "id": "w6qqYKt4LYtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(model, hifigan_vocoder, waveform, device):\n",
        "    # Do not change the code\n",
        "\n",
        "    model.eval()\n",
        "    hifigan_vocoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Ensure proper shape and move to device\n",
        "        if waveform.dim() == 2:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "        waveform = waveform.to(device)\n",
        "\n",
        "        # Generate log-mel spectrogram from input waveform\n",
        "        log_mel_spec = model(waveform)\n",
        "\n",
        "        # Scale the log-mel spectrogram to match HiFi-GAN's expected input range\n",
        "        if log_mel_spec.min() < 0:\n",
        "            log_mel_spec = (log_mel_spec + 1) / 2\n",
        "\n",
        "        # Generate audio from log-mel spectrogram using HiFi-GAN\n",
        "        generated_audio = hifigan_vocoder(log_mel_spec)\n",
        "    return generated_audio.squeeze().cpu(), log_mel_spec.cpu()"
      ],
      "metadata": {
        "id": "EUn5sNZ1LWz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_model(model_path):\n",
        "    loaded = torch.load(model_path, map_location='cpu')\n",
        "    print(f\"Type of loaded object: {type(loaded)}\")\n",
        "\n",
        "    if isinstance(loaded, dict):\n",
        "        print(\"Keys in the dictionary:\")\n",
        "        for key in loaded.keys():\n",
        "            print(f\"- {key}: {type(loaded[key])}\")\n",
        "\n",
        "            # If it's another dictionary or has state_dict\n",
        "            if isinstance(loaded[key], dict):\n",
        "                print(f\"  Nested keys in {key}:\")\n",
        "                for nested_key in loaded[key].keys():\n",
        "                    print(f\"  - {nested_key}\")\n",
        "            elif hasattr(loaded[key], 'state_dict'):\n",
        "                print(f\"  Has state_dict with keys:\")\n",
        "                for state_key in loaded[key].state_dict().keys():\n",
        "                    print(f\"  - {state_key}\")\n",
        "\n",
        "inspect_model(\"eeg_to_mel_model_final.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "jOM2JSrSzlYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa36a704-709b-49c1-b600-43c086c501f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of loaded object: <class 'dict'>\n",
            "Keys in the dictionary:\n",
            "- model_state_dict: <class 'collections.OrderedDict'>\n",
            "  Nested keys in model_state_dict:\n",
            "  - fc1.weight\n",
            "  - fc1.bias\n",
            "  - bn1.weight\n",
            "  - bn1.bias\n",
            "  - bn1.running_mean\n",
            "  - bn1.running_var\n",
            "  - bn1.num_batches_tracked\n",
            "  - fc2.weight\n",
            "  - fc2.bias\n",
            "  - bn2.weight\n",
            "  - bn2.bias\n",
            "  - bn2.running_mean\n",
            "  - bn2.running_var\n",
            "  - bn2.num_batches_tracked\n",
            "  - fc3.weight\n",
            "  - fc3.bias\n",
            "  - bn3.weight\n",
            "  - bn3.bias\n",
            "  - bn3.running_mean\n",
            "  - bn3.running_var\n",
            "  - bn3.num_batches_tracked\n",
            "  - output.weight\n",
            "  - output.bias\n",
            "- input_dim: <class 'int'>\n",
            "- hidden_dim: <class 'int'>\n",
            "- output_dim: <class 'int'>\n",
            "- mel_channels: <class 'int'>\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3twB_sQWCijn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}